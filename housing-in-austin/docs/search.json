[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Introduction",
    "section": "",
    "text": "Austin, Texas, has emerged as one of the most dynamic and rapidly growing cities in the United States, experiencing a substantial influx of residents and job opportunities in recent years. However, this rapid growth has strained the housing market and construction, leading to a significant disparity between supply and demand. Consequently, the housing prices in Austin have faced a steep upward trend.\nRecognizing the challenges posed by this scenario, for our MUSA 550 final project we propose the development of a comprehensive project aimed at modeling the housing market and predicting housing prices based on a diverse set of housing characteristics. This initiative is designed to provide valuable insights for users, inform policy decisions, and assist consultants in navigating the evolving real estate landscape in Austin."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "",
    "text": "This is the website of our final project for MUSA 550 at Penn."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "",
    "text": "This is the website of our final project for MUSA 550 at Penn."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Find out more",
    "text": "Find out more\nThe code for this project is hosted on the GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "",
    "text": "Web Scraping Zillow Apartments: Extracting real-time housing data from Zillow will provide a wealth of information on current property listings, sales history, and key features of residences in Austin. This data will serve as a foundational element for our predictive model.\nCensus API Demographic Data: Utilizing the Census API will enable us to incorporate demographic information, including population trends, income levels, and household composition.\nOpen Data Portal Geographic Information: Accessing geographic information from open data portals will enrich our model with location-specific details. This includes factors such as neighborhood amenities, crime rates, and proximity to essential services, providing a holistic view of the housing market dynamics.\n\n\n\n\n\n\nImportant\n\n\n\nAll data acquired complies with regulations."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Methodology",
    "text": "Methodology\nOur approach will involve training a Random Forest model, which will be trained using the joined dataset, encompassing a diverse range of block-level demographic, geographical, and housing characteristics."
  },
  {
    "objectID": "datamethod.html",
    "href": "datamethod.html",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "datamethod.html#welcome",
    "href": "datamethod.html#welcome",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "datamethod.html#find-out-more",
    "href": "datamethod.html#find-out-more",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "datamethodpage.html",
    "href": "datamethodpage.html",
    "title": "Data and Methodology",
    "section": "",
    "text": "Web Scraping Zillow Apartments\nExtracting real-time housing data from Zillow will provide a wealth of information on current property listings, sales history, and key features of residences in Austin. This data will serve as a foundational element for our predictive model.\nCensus API Demographic Data\nUtilizing the Census API will enable us to incorporate demographic information, including population trends, income levels, and household composition.\nOpen Data Portal Geographic Information\nAccessing geographic information from open data portals will enrich our model with location-specific details. This includes factors such as neighborhood amenities, crime rates, and proximity to essential services, providing a holistic view of the housing market dynamics.\n\n\n\n\n\n\nImportant\n\n\n\nAll data acquired complies with regulations."
  },
  {
    "objectID": "datamethodpage.html#data-source",
    "href": "datamethodpage.html#data-source",
    "title": "Data and Methodology",
    "section": "",
    "text": "Web Scraping Zillow Apartments\nExtracting real-time housing data from Zillow will provide a wealth of information on current property listings, sales history, and key features of residences in Austin. This data will serve as a foundational element for our predictive model.\nCensus API Demographic Data\nUtilizing the Census API will enable us to incorporate demographic information, including population trends, income levels, and household composition.\nOpen Data Portal Geographic Information\nAccessing geographic information from open data portals will enrich our model with location-specific details. This includes factors such as neighborhood amenities, crime rates, and proximity to essential services, providing a holistic view of the housing market dynamics.\n\n\n\n\n\n\nImportant\n\n\n\nAll data acquired complies with regulations."
  },
  {
    "objectID": "datamethodpage.html#methodology",
    "href": "datamethodpage.html#methodology",
    "title": "Data and Methodology",
    "section": "Methodology",
    "text": "Methodology\nOur approach will involve training a Random Forest model, which will be trained using the joined dataset, encompassing a diverse range of block-level demographic, geographical, and housing characteristics."
  },
  {
    "objectID": "index.html#contact-us-to-learn-more",
    "href": "index.html#contact-us-to-learn-more",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Contact us to learn more",
    "text": "Contact us to learn more"
  },
  {
    "objectID": "index.html#meet-the-team",
    "href": "index.html#meet-the-team",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Meet the team",
    "text": "Meet the team\nWe are second-year graduate students majoring in City Planning with a concentration in transportation planning.\nEvan Zhao\nKe Zhou"
  },
  {
    "objectID": "index.html#contact-us",
    "href": "index.html#contact-us",
    "title": "Predictive Modeling of Housing Prices in Austin, Texas",
    "section": "Contact us",
    "text": "Contact us\nKe Zhou : kezhou@upenn.edu\nEvan Zhao : yanbing@upenn.edu"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Austin, Texas, has undergone remarkable growth, emerging as one of the most dynamic cities in the United States, attracting a substantial influx of residents and job opportunities. However, this rapid expansion has placed immense strain on the local housing market and construction industry, resulting in a pronounced imbalance between supply and demand. As a consequence, housing prices in Austin have witnessed a sharp upward trajectory.\nIn response to these challenges, we developed a robust model with a mean R-square value of 0.737. This model leverages a comprehensive dataset encompassing Zillow housing listings, crime reports, school data, census information, and numerous engineered variables. Our goal is to provide predictions of housing prices, empowering users to make well-informed decisions regarding both rental and purchase considerations.\nBeyond individual decision-making, our model offers valuable insights for policymakers grappling with the complex dynamics of Austin’s housing market. By understanding the underlying factors influencing housing price trends, policymakers can formulate informed strategies to address the evolving needs of the community and promote sustainable growth."
  },
  {
    "objectID": "exploratory/1-zillow.html",
    "href": "exploratory/1-zillow.html",
    "title": "Data Transformation and Analysis",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of xxxx.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\n\n# DATA 1 : Zillow Housing Data\ndriver = webdriver.Chrome()\nbase_url = 'https://www.zillow.com/austin-tx/'\ndriver.get(base_url)\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\naddress = driver.find_elements(By.XPATH,'//address')\nprice = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\narea = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\nbath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\nbed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\naddress_scraped=[]\nprice_scraped=[]\narea_scraped = []\nbath_scraped = []\nbed_scraped = []\n\nfor result in address:\n    address_scraped.append(result.text)\nfor result in price:\n    price_scraped.append(result.text)\nfor result in area:\n    area_scraped.append(result.text)\nfor result in bath:\n    bath_scraped.append(result.text)\nfor result in bed:\n    bed_scraped.append(result.text)\n    \nwith open(\"zillow2.csv\", \"w\") as f:\n    f.write(\"Address; Price; Area; Bath; Bed\\n\")\nfor i in range(len(address_scraped)):\n    with open(\"zillow2.csv\", \"a\") as f:\n        f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport pandas\nhousing = pandas.read_csv(\"zillow2.csv\", delimiter=\";\")\nhousing.head(10)\n\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\n\n\n\n\n\n\n\n\n\n\nNext, we perform geocoding using geocoders for the scraped addresses to obtain the locations and coordinates of each housing unit. This step is essential for preparing the data for future spatial analysis. After geocoding and tranformations based on the addresses, we acquired a dataset with coordiantes. In the process there were some data loss since the Open Street Map couldn’t find location matches with some addresses.\n\n\nCode\n#! code-fold: true\n\nfrom geopy.geocoders import Nominatim\nnom = Nominatim(user_agent=\"housing\")\nhousing[\"coordinates\"] = housing[\"Address\"].apply(nom.geocode)\nhousing[\"longitude\"] = housing[\"coordinates\"].apply(lambda x:x.longitude if x != None else None)\nhousing[\"latitude\"] = housing[\"coordinates\"].apply(lambda x:x.latitude if x != None else None)\n\nimport geopandas as gpd\ngeometry = gpd.points_from_xy(housing[\"longitude\"], housing[\"latitude\"])\nhousing_geo = gpd.GeoDataFrame(housing, geometry=geometry)\n\nhousing_geo.columns = housing_geo.columns.str.strip()\n\n#to float\nhousing_geo[\"price\"] = housing_geo[\"Price\"].replace('[\\$,+]', \"\", regex=True).astype(float)\nhousing_geo[\"area\"] = housing_geo[\"Area\"].replace(\"[\\,]\", \"\", regex=True).astype(float)\nhousing_geo[\"bed\"] = housing_geo[\"Bed\"].astype(float)\nhousing_geo[\"bath\"] = housing_geo[\"Bath\"].astype(float)\n\nhousing_geo.head(10)\n\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\ncoordinates\nlongitude\nlatitude\ngeometry\nprice\narea\nbed\nbath\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n(3504, Black Granite Drive, Colorado Crossing,...\n-97.697120\n30.202828\nPOINT (-97.69712 30.20283)\n350000.0\n1463.0\n3.0\n2.0\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n109995.0\n1216.0\n3.0\n2.0\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n(9717, Meadowheath Drive, Austin, Williamson C...\n-97.785724\n30.459727\nPOINT (-97.78572 30.45973)\n339900.0\n1420.0\n3.0\n2.0\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n323000.0\n1069.0\n3.0\n2.0\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\nNone\nNaN\nNaN\nPOINT EMPTY\n374998.0\n2451.0\n4.0\n3.0\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n(4509, Kind Way, Chaparral Crossing, Austin, T...\n-97.573388\n30.237360\nPOINT (-97.57339 30.23736)\n279000.0\n1302.0\n3.0\n3.0\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n(9017, Frostwood Trail, Austin, Williamson Cou...\n-97.774245\n30.459452\nPOINT (-97.77425 30.45945)\n450000.0\n2709.0\n4.0\n3.0\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\nNone\nNaN\nNaN\nPOINT EMPTY\n194898.0\n435.0\n1.0\n1.0\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n124900.0\n1568.0\n3.0\n2.0\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\nNone\nNaN\nNaN\nPOINT EMPTY\n169900.0\n802.0\n2.0\n2.0\n\n\n\n\n\n\n\n\n\n\nAfter condcuting some exploratory analysis on\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport altair as alt\n\nselected_columns =  ['price', 'area', 'bed', 'Address']\n\nchart = alt.Chart(housing_geo[selected_columns]).mark_circle().encode(\n    x=alt.X('price:Q', axis=alt.Axis(title='Price')),\n    y=alt.Y('area:Q', axis=alt.Axis(title='Size')),\n    color=alt.Color('bed:Q', legend=alt.Legend(title='Bedrooms')),\n    tooltip=['price:Q', 'area:Q', 'bed:Q', 'Address:N']\n).properties(width=600, height=400)\n\nchart.interactive()\n\n\n\n\n\n\n\n\nThe average housing price per suqare feet in Austin is around xxx, as indicated by the chart, whcih is relatively higher than the U.S. average.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport matplotlib.pyplot as plt\nprice = housing_geo['price']\narea = housing_geo['area']\n\nplt.hist(price/area, bins=15, color='#A5DEF2')\nplt.title('Austin Housing Prices per square foot')\nplt.xlabel('Housing Prices per square foot')\nplt.ylabel('Frequency')\n\nplt.show()"
  },
  {
    "objectID": "exploratory/1-zillow.html#geocoding-for-zillow-address",
    "href": "exploratory/1-zillow.html#geocoding-for-zillow-address",
    "title": "1. Zillow Data",
    "section": "1.2 Geocoding for Zillow Address",
    "text": "1.2 Geocoding for Zillow Address\nNext, we perform geocoding using geocoders for the scraped addresses to obtain the locations and coordinates of each housing unit. This step is essential for preparing the data for future spatial analysis. First we will look at the scraped housing data:\n\n#! echo: true\n#! code-fold: true\n\nimport pandas\nhousing = pandas.read_csv(\"zillow2.csv\", delimiter=\";\")\nhousing.head(10)\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\n\n\n\n\n\n\n\nThen after geocoding and tranformations based on the addresses, we acquired a dataset with coordiantes. In the process there were some data loss since the Open Street Map couldn’t find location matches with some addresses.\n\n#! echo: true\n#! code-fold: true\n\nfrom geopy.geocoders import Nominatim\nnom = Nominatim(user_agent=\"housing\")\nhousing[\"coordinates\"] = housing[\"Address\"].apply(nom.geocode)\nhousing[\"longitude\"] = housing[\"coordinates\"].apply(lambda x:x.longitude if x != None else None)\nhousing[\"latitude\"] = housing[\"coordinates\"].apply(lambda x:x.latitude if x != None else None)\n\nimport geopandas as gpd\ngeometry = gpd.points_from_xy(housing[\"longitude\"], housing[\"latitude\"])\nhousing_geo = gpd.GeoDataFrame(housing, geometry=geometry)\n\nhousing_geo.columns = housing_geo.columns.str.strip()\n\n#to float\nhousing_geo[\"price\"] = housing_geo[\"Price\"].replace('[\\$,+]', \"\", regex=True).astype(float)\nhousing_geo[\"area\"] = housing_geo[\"Area\"].replace(\"[\\,]\", \"\", regex=True).astype(float)\nhousing_geo[\"bed\"] = housing_geo[\"Bed\"].astype(float)\nhousing_geo[\"bath\"] = housing_geo[\"Bath\"].astype(float)\n\nhousing_geo.head(10)\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\ncoordinates\nlongitude\nlatitude\ngeometry\nprice\narea\nbed\nbath\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n(3504, Black Granite Drive, Colorado Crossing,...\n-97.697120\n30.202828\nPOINT (-97.69712 30.20283)\n350000.0\n1463.0\n3.0\n2.0\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n109995.0\n1216.0\n3.0\n2.0\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n(9717, Meadowheath Drive, Austin, Williamson C...\n-97.785724\n30.459727\nPOINT (-97.78572 30.45973)\n339900.0\n1420.0\n3.0\n2.0\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n323000.0\n1069.0\n3.0\n2.0\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\nNone\nNaN\nNaN\nPOINT EMPTY\n374998.0\n2451.0\n4.0\n3.0\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n(4509, Kind Way, Chaparral Crossing, Austin, T...\n-97.573388\n30.237360\nPOINT (-97.57339 30.23736)\n279000.0\n1302.0\n3.0\n3.0\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n(9017, Frostwood Trail, Austin, Williamson Cou...\n-97.774245\n30.459452\nPOINT (-97.77425 30.45945)\n450000.0\n2709.0\n4.0\n3.0\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\nNone\nNaN\nNaN\nPOINT EMPTY\n194898.0\n435.0\n1.0\n1.0\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n124900.0\n1568.0\n3.0\n2.0\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\nNone\nNaN\nNaN\nPOINT EMPTY\n169900.0\n802.0\n2.0\n2.0"
  },
  {
    "objectID": "exploratory/1-zillow.html#web-scrape-housing-lsits-on-zillow",
    "href": "exploratory/1-zillow.html#web-scrape-housing-lsits-on-zillow",
    "title": "Zillow Data",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of xxxx.\n\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\n\n# DATA 1 : Zillow Housing Data\ndriver = webdriver.Chrome()\nbase_url = 'https://www.zillow.com/austin-tx/'\ndriver.get(base_url)\n\n\n#! echo: true\n#! code-fold: true\n\naddress = driver.find_elements(By.XPATH,'//address')\nprice = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\narea = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\nbath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\nbed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\naddress_scraped=[]\nprice_scraped=[]\narea_scraped = []\nbath_scraped = []\nbed_scraped = []\n\nfor result in address:\n    address_scraped.append(result.text)\nfor result in price:\n    price_scraped.append(result.text)\nfor result in area:\n    area_scraped.append(result.text)\nfor result in bath:\n    bath_scraped.append(result.text)\nfor result in bed:\n    bed_scraped.append(result.text)\n    \nwith open(\"zillow2.csv\", \"w\") as f:\n    f.write(\"Address; Price; Area; Bath; Bed\\n\")\nfor i in range(len(address_scraped)):\n    with open(\"zillow2.csv\", \"a\") as f:\n        f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")"
  },
  {
    "objectID": "exploratory/1-zillow.html#web-scrape-housing-lists-on-zillow",
    "href": "exploratory/1-zillow.html#web-scrape-housing-lists-on-zillow",
    "title": "1. Zillow Data",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of xxxx.\n\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\n\n# DATA 1 : Zillow Housing Data\ndriver = webdriver.Chrome()\nbase_url = 'https://www.zillow.com/austin-tx/'\ndriver.get(base_url)\n\n\n#! echo: true\n#! code-fold: true\n\naddress = driver.find_elements(By.XPATH,'//address')\nprice = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\narea = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\nbath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\nbed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\naddress_scraped=[]\nprice_scraped=[]\narea_scraped = []\nbath_scraped = []\nbed_scraped = []\n\nfor result in address:\n    address_scraped.append(result.text)\nfor result in price:\n    price_scraped.append(result.text)\nfor result in area:\n    area_scraped.append(result.text)\nfor result in bath:\n    bath_scraped.append(result.text)\nfor result in bed:\n    bed_scraped.append(result.text)\n    \nwith open(\"zillow2.csv\", \"w\") as f:\n    f.write(\"Address; Price; Area; Bath; Bed\\n\")\nfor i in range(len(address_scraped)):\n    with open(\"zillow2.csv\", \"a\") as f:\n        f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")"
  },
  {
    "objectID": "exploratory/1-zillow.html#exploratory-analysis-on-zillow-housing",
    "href": "exploratory/1-zillow.html#exploratory-analysis-on-zillow-housing",
    "title": "1. Zillow Data",
    "section": "1.3 Exploratory Analysis on Zillow Housing",
    "text": "1.3 Exploratory Analysis on Zillow Housing\nAfter condcuting some exploratory analysis on\n\n#! echo: true\n#! code-fold: true\n\nimport altair as alt\n\nselected_columns =  ['price', 'area', 'bed', 'Address']\n\nchart = alt.Chart(housing_geo[selected_columns]).mark_circle().encode(\n    x=alt.X('price:Q', axis=alt.Axis(title='Price')),\n    y=alt.Y('area:Q', axis=alt.Axis(title='Size')),\n    color=alt.Color('bed:Q', legend=alt.Legend(title='Bedrooms')),\n    tooltip=['price:Q', 'area:Q', 'bed:Q', 'Address:N']\n).properties(width=600, height=400)\n\nchart.interactive()\n\n\n\n\n\n\n\nThe average housing price per suqare feet in Austin is around xxx, as indicated by the chart, whcih is relatively higher than the U.S. average.\n\n#! echo: true\n#! code-fold: true\n\nimport matplotlib.pyplot as plt\nprice = housing_geo['price']\narea = housing_geo['area']\n\nplt.hist(price/area, bins=15, color='#A5DEF2')\nplt.title('Austin Housing Prices per square foot')\nplt.xlabel('Housing Prices per square foot')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\n\n\n\n# 2 Trcat Data\n\nimport pygris\n\naus_tracts = pygris.tracts(\n    state=\"48\", county=\"453\", year=2021\n)\n\nfig, ax = plt.subplots(figsize=(8, 8))\naus_tracts.plot(ax=ax, facecolor='none', edgecolor='black', linestyle='-', lw=0.5)\nplt.title('Austin Census Tract')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exploratory/2-tract.html",
    "href": "exploratory/2-tract.html",
    "title": "MUSA 550 Quarto Template",
    "section": "",
    "text": "##Tract Data\nNext we acquire the census tract spatial data from pygris.\n\nimport pygris\n\naus_tracts = pygris.tracts(\n    state=\"48\", county=\"453\", year=2021\n)\n\nfig, ax = plt.subplots(figsize=(8, 8))\naus_tracts.plot(ax=ax, facecolor='none', edgecolor='black', linestyle='-', lw=0.5)\nplt.title('Austin Census Tract')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exploratory/index_ex.html",
    "href": "exploratory/index_ex.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Data Exploration\nThis section includes acuiring and transformig the data of housing, demographics and other spatial characteristics, and some exploratory analysis to better understand the existing conditions of housing markets in Austin."
  },
  {
    "objectID": "exploratory/1-zillow.html#zillow-data",
    "href": "exploratory/1-zillow.html#zillow-data",
    "title": "Data Transformation and Analysis",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of xxxx.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\n\n# DATA 1 : Zillow Housing Data\ndriver = webdriver.Chrome()\nbase_url = 'https://www.zillow.com/austin-tx/'\ndriver.get(base_url)\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\naddress = driver.find_elements(By.XPATH,'//address')\nprice = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\narea = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\nbath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\nbed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\naddress_scraped=[]\nprice_scraped=[]\narea_scraped = []\nbath_scraped = []\nbed_scraped = []\n\nfor result in address:\n    address_scraped.append(result.text)\nfor result in price:\n    price_scraped.append(result.text)\nfor result in area:\n    area_scraped.append(result.text)\nfor result in bath:\n    bath_scraped.append(result.text)\nfor result in bed:\n    bed_scraped.append(result.text)\n    \nwith open(\"zillow2.csv\", \"w\") as f:\n    f.write(\"Address; Price; Area; Bath; Bed\\n\")\nfor i in range(len(address_scraped)):\n    with open(\"zillow2.csv\", \"a\") as f:\n        f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport pandas\nhousing = pandas.read_csv(\"zillow2.csv\", delimiter=\";\")\nhousing.head(10)\n\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\n\n\n\n\n\n\n\n\n\n\nNext, we perform geocoding using geocoders for the scraped addresses to obtain the locations and coordinates of each housing unit. This step is essential for preparing the data for future spatial analysis. After geocoding and tranformations based on the addresses, we acquired a dataset with coordiantes. In the process there were some data loss since the Open Street Map couldn’t find location matches with some addresses.\n\n\nCode\n#! code-fold: true\n\nfrom geopy.geocoders import Nominatim\nnom = Nominatim(user_agent=\"housing\")\nhousing[\"coordinates\"] = housing[\"Address\"].apply(nom.geocode)\nhousing[\"longitude\"] = housing[\"coordinates\"].apply(lambda x:x.longitude if x != None else None)\nhousing[\"latitude\"] = housing[\"coordinates\"].apply(lambda x:x.latitude if x != None else None)\n\nimport geopandas as gpd\ngeometry = gpd.points_from_xy(housing[\"longitude\"], housing[\"latitude\"])\nhousing_geo = gpd.GeoDataFrame(housing, geometry=geometry)\n\nhousing_geo.columns = housing_geo.columns.str.strip()\n\n#to float\nhousing_geo[\"price\"] = housing_geo[\"Price\"].replace('[\\$,+]', \"\", regex=True).astype(float)\nhousing_geo[\"area\"] = housing_geo[\"Area\"].replace(\"[\\,]\", \"\", regex=True).astype(float)\nhousing_geo[\"bed\"] = housing_geo[\"Bed\"].astype(float)\nhousing_geo[\"bath\"] = housing_geo[\"Bath\"].astype(float)\n\nhousing_geo.head(10)\n\n\n\n\n\n\n\n\n\nAddress\nPrice\nArea\nBath\nBed\ncoordinates\nlongitude\nlatitude\ngeometry\nprice\narea\nbed\nbath\n\n\n\n\n0\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n1,463\n2\n3\n(3504, Black Granite Drive, Colorado Crossing,...\n-97.697120\n30.202828\nPOINT (-97.69712 30.20283)\n350000.0\n1463.0\n3.0\n2.0\n\n\n1\n7601 Daffan Lane #047 Plan, Oak Crest\n$109,995+\n1,216\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n109995.0\n1216.0\n3.0\n2.0\n\n\n2\n9717 Meadowheath Dr, Austin, TX 78729\n$339,900\n1,420\n2\n3\n(9717, Meadowheath Drive, Austin, Williamson C...\n-97.785724\n30.459727\nPOINT (-97.78572 30.45973)\n339900.0\n1420.0\n3.0\n2.0\n\n\n3\n10506 Little Pebble Dr #A, Austin, TX 78758\n$323,000\n1,069\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n323000.0\n1069.0\n3.0\n2.0\n\n\n4\n1618 Lawnside Rd, San Antonio, TX 78245\n$374,998\n2,451\n3\n4\nNone\nNaN\nNaN\nPOINT EMPTY\n374998.0\n2451.0\n4.0\n3.0\n\n\n5\n4509 Kind Way, Austin, TX 78725\n$279,000\n1,302\n3\n3\n(4509, Kind Way, Chaparral Crossing, Austin, T...\n-97.573388\n30.237360\nPOINT (-97.57339 30.23736)\n279000.0\n1302.0\n3.0\n3.0\n\n\n6\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n2,709\n3\n4\n(9017, Frostwood Trail, Austin, Williamson Cou...\n-97.774245\n30.459452\nPOINT (-97.77425 30.45945)\n450000.0\n2709.0\n4.0\n3.0\n\n\n7\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n435\n1\n1\nNone\nNaN\nNaN\nPOINT EMPTY\n194898.0\n435.0\n1.0\n1.0\n\n\n8\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n1,568\n2\n3\nNone\nNaN\nNaN\nPOINT EMPTY\n124900.0\n1568.0\n3.0\n2.0\n\n\n9\n2124 Burton Dr APT 127, Austin, TX 78741\n$169,900\n802\n2\n2\nNone\nNaN\nNaN\nPOINT EMPTY\n169900.0\n802.0\n2.0\n2.0\n\n\n\n\n\n\n\n\n\n\nAfter condcuting some exploratory analysis on\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport altair as alt\n\nselected_columns =  ['price', 'area', 'bed', 'Address']\n\nchart = alt.Chart(housing_geo[selected_columns]).mark_circle().encode(\n    x=alt.X('price:Q', axis=alt.Axis(title='Price')),\n    y=alt.Y('area:Q', axis=alt.Axis(title='Size')),\n    color=alt.Color('bed:Q', legend=alt.Legend(title='Bedrooms')),\n    tooltip=['price:Q', 'area:Q', 'bed:Q', 'Address:N']\n).properties(width=600, height=400)\n\nchart.interactive()\n\n\n\n\n\n\n\n\nThe average housing price per suqare feet in Austin is around xxx, as indicated by the chart, whcih is relatively higher than the U.S. average.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport matplotlib.pyplot as plt\nprice = housing_geo['price']\narea = housing_geo['area']\n\nplt.hist(price/area, bins=15, color='#A5DEF2')\nplt.title('Austin Housing Prices per square foot')\nplt.xlabel('Housing Prices per square foot')\nplt.ylabel('Frequency')\n\nplt.show()"
  },
  {
    "objectID": "exploratory/1-zillow.html#tract-data",
    "href": "exploratory/1-zillow.html#tract-data",
    "title": "Data Transformation and Analysis",
    "section": "2. Tract Data",
    "text": "2. Tract Data\nThen we extracted census tract data to provide spatial references in Austin.\n\n\nCode\nimport pygris\n\naus_tracts = pygris.tracts(\n    state=\"48\", county=\"453\", year=2021\n)\n\nfig, ax = plt.subplots(figsize=(8, 8))\naus_tracts.plot(ax=ax, facecolor='none', edgecolor='black', linestyle='-', lw=0.5)\nplt.title('Austin Census Tract')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exploratory/1-zillow.html#census-and-demographic-data",
    "href": "exploratory/1-zillow.html#census-and-demographic-data",
    "title": "Data Transformation and Analysis",
    "section": "3. Census and Demographic Data",
    "text": "3. Census and Demographic Data\nTo gain a deeper understanding of how the surrounding neighborhood demographics and characteristics influence housing prices in Austin, we obtained census data from the Census API and spatially joiend with tract data. This data will serve as a valuable resource to explore and analyze the relationships between various demographic factors and housing prices in the area.\n\n3.1 Median Income\nIn our analysis, we prioritized the examination of median income as the initial demographic variable. Median income is a key metric that provides valuable insights into the economic prosperity and financial health of a specific population. By focusing on median income, we aim to understand the distribution of earnings within the community and its potential impact on housing dynamics.\n\n\nCode\nimport cenpy\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nincome = acs.varslike(\"MEDIAN HOUSEHOLD INCOME\",\n                      by=\"concept\",\n).sort_index()\n\nvariables = [\n    \"NAME\",\n    \"B19013A_001E\"]\n\naus_data = acs.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": \"48\", \"county\": \"453\"},\n)\n\nfor variable in variables:\n    if variable != \"NAME\":\n        aus_data[variable] = aus_data[variable].astype(float)\n\nmerged = pandas.merge(aus_data, aus_tracts, left_on=\"tract\", right_on=\"TRACTCE\", how=\"right\")\nmask = merged['B19013A_001E'] &gt;= 0\nmerged = merged[mask]\n\nmerged = merged.rename(columns={\"B19013A_001E\" : \"median_income\"})\nmerged.head()\n\n\n\n\n\n\n\n\n\nNAME_x\nmedian_income\nstate\ncounty\ntract\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\nNAME_y\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\nCensus Tract 22.11, Travis County, Texas\n83882.0\n48\n453\n002211\n48\n453\n002211\n48453002211\n22.11\nCensus Tract 22.11\nG5020\nS\n6799964\n0\n+30.2742994\n-097.6192562\nPOLYGON ((-97.63835 30.27447, -97.63727 30.276...\n\n\n1\nCensus Tract 25, Travis County, Texas\n85822.0\n48\n453\n002500\n48\n453\n002500\n48453002500\n25\nCensus Tract 25\nG5020\nS\n3419935\n0\n+30.4166759\n-097.7551447\nPOLYGON ((-97.76760 30.41209, -97.76753 30.412...\n\n\n2\nCensus Tract 24.36, Travis County, Texas\n53214.0\n48\n453\n002436\n48\n453\n002436\n48453002436\n24.36\nCensus Tract 24.36\nG5020\nS\n57384988\n13\n+30.1065025\n-097.6591614\nPOLYGON ((-97.69697 30.14765, -97.69691 30.149...\n\n\n3\nCensus Tract 24.32, Travis County, Texas\n110221.0\n48\n453\n002432\n48\n453\n002432\n48453002432\n24.32\nCensus Tract 24.32\nG5020\nS\n44469976\n14350\n+30.1302451\n-097.7195595\nPOLYGON ((-97.75577 30.10118, -97.75562 30.101...\n\n\n4\nCensus Tract 21.12, Travis County, Texas\n80553.0\n48\n453\n002112\n48\n453\n002112\n48453002112\n21.12\nCensus Tract 21.12\nG5020\nS\n1703455\n0\n+30.3139236\n-097.6825978\nPOLYGON ((-97.69126 30.31622, -97.69081 30.316...\n\n\n\n\n\n\n\nNext we joined the housing locations to the demographic dataset, with the two inforamtion layered, we can observe certain patterns.\n\n\nCode\n## Join housisng data to the tract it belongs to\nmerged = gpd.GeoDataFrame(merged, geometry = 'geometry')\n\nhousing_geo.crs = \"EPSG:4326\"\nmerged = merged.to_crs(housing_geo.crs)\nhousing_dem_geo = gpd.sjoin(housing_geo, merged, how=\"left\", op=\"within\")\n\n#layerd with neighborhood income data\nfig, ax = plt.subplots(figsize=(8, 8))\n\nmerged.plot(ax=ax, \n            column='median_income', \n            cmap='Blues', \n            legend=True, \n            edgecolor='black', \n            linestyle='-', \n            lw=0.5, \n            legend_kwds={'label': \"Median Income\"})\n\nhousing_dem_geo.plot(ax=ax, \n                     column='price', \n                     cmap='RdPu', \n                     legend=True, \n                     edgecolor='black', \n                     linestyle='-', \n                     lw=0.2, \n                     vmax=1400000, \n                     legend_kwds={'label': \"Price\"})\n\nplt.title('Housing Price and Neighborhood Median Income')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\nD:\\ana\\envs\\environment\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):"
  },
  {
    "objectID": "Modeling/index_ex2.html",
    "href": "Modeling/index_ex2.html",
    "title": "Random Forest Model",
    "section": "",
    "text": "Random Forest Model\nThis section includes the model building process using the dataset we generated to explore the realtionships between various varibles and housing prices in Austin in order to predict housing prices more acurrately."
  },
  {
    "objectID": "Modeling/1-modelbuild.html",
    "href": "Modeling/1-modelbuild.html",
    "title": "Modeling",
    "section": "",
    "text": "Utilizing a comprehensive dataset amalgamating Zillow housing listings, crime reports, school information, census data, and various engineered variables, we proceeded to select the most relevant features for constructing our housing price model using the Random Forest algorithm.\nCode\n#! echo: true\n#! code-fold: true\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nimport geopandas as gpd\n\nhousing_dem_geo = gpd.read_file(\"../exploratory/final_dataset.geojson\")\n\ncols = [\n    \"price\",\n    \"area\",\n    \"bed\",\n    \"bath\",\n    \"median_income\",\n    \"avg_dist_nearest_crimes\",\n    \"nearest_school_rating\",\n    \"neighname\",\n]\n\n# Trim to these columns and remove NaNsneigh\nrf_housing = housing_dem_geo[cols + [\"geometry\"]].dropna()\nrf_housing.head()\n\n\n\n\n\n\n\n\n\nprice\narea\nbed\nbath\nmedian_income\navg_dist_nearest_crimes\nnearest_school_rating\nneighname\ngeometry\n\n\n\n\n0\n610000.0\n3260.0\n5.0\n4.0\n102857.0\n0.005493\n4.4\nSLAUGHTER CREEK\nPOINT (-97.81409 30.14998)\n\n\n1\n350000.0\n1463.0\n3.0\n2.0\n96563.0\n0.007604\n3.6\nSOUTHEAST\nPOINT (-97.69715 30.20284)\n\n\n2\n124900.0\n1568.0\n3.0\n2.0\n79698.0\n0.005222\n4.4\nBLUFF SPRINGS\nPOINT (-97.76396 30.17374)\n\n\n3\n194898.0\n435.0\n1.0\n1.0\n99784.0\n0.004489\n3.6\nBOULDIN CREEK\nPOINT (-97.75573 30.24913)\n\n\n4\n259900.0\n1084.0\n2.0\n2.0\n63750.0\n0.010090\n4.4\nBLUFF SPRINGS\nPOINT (-97.73749 30.15103)\nNext we conducted a corrlation matrix to explore relationships between different varibales and avoid colinearity.\nCode\n#! echo: true\n#! code-fold: true\n\n#correlation matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnum_cols = [\n    \"price\",\n    \"area\",\n    \"bed\",\n    \"bath\",\n    \"median_income\",\n    \"avg_dist_nearest_crimes\",\n    \"nearest_school_rating\",\n]\n\n\ncorrelation_matrix = rf_housing[num_cols].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=sns.diverging_palette(5, 250, as_cmap=True), fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Matrix\")\nplt.show()\nWe initially categorized the variables into two groups: numeric columns and categorical columns. Subsequently, we partitioned the dataset into training and test sets, allocating 30% for testing and 70% for training. Following a series of experiments, we successfully constructed a model that achieved a mean R2 score of 0.737.\nCode\n#! echo: true\n#! code-fold: true\n\n# Numerical columns\nnum_cols = [\n    \"area\",\n    \"bed\",\n    \"bath\",\n    \"median_income\",\n    \"avg_dist_nearest_crimes\",\n    \"nearest_school_rating\",\n]\n\n# Categorical columns if there are some\ncat_cols = [\"neighname\"]\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=10, \n                                       random_state=42)\n)\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(rf_housing, test_size=0.3, random_state=42)\n\ny_train = np.log(train_set[\"price\"])\ny_test = np.log(test_set[\"price\"])\n\nfeature_cols = [\n    \"area\",\n    \"bed\",\n    \"bath\",\n    \"median_income\",\n    \"avg_dist_nearest_crimes\",\n    \"nearest_school_rating\",\n    \"neighname\",\n]\n\nX_train = train_set[feature_cols]\nX_test = test_set[feature_cols]\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nscores = cross_val_score(\n    pipe,\n    X_train,\n    y_train,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.73411372 0.74268769 0.80581977 0.7091332  0.644016   0.80281966\n 0.71928379 0.66465207 0.66383977 0.78393809]\nScores mean =  0.7270303741111018\nScore std dev =  0.05535468147785206\nFinally, we get the random forest model of a pipe score of 0.7877\nCode\npipe.fit(train_set, y_train);\npipe.score(test_set, y_test)\n\n\n0.7844674238125384\nTo improve the model, we did a grid search to find out the best parameters for our model.\nCode\n#! echo: true\n#! code-fold: true\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n\ngrid.fit(X_train, y_train)\n\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['area',\n                                                                          'bed',\n                                                                          'bath',\n                                                                          'median_income',\n                                                                          'avg_dist_nearest_crimes',\n                                                                          'nearest_school_rating']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['neighname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(n_estimators=10,\n                                                              random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['area',\n                                                                          'bed',\n                                                                          'bath',\n                                                                          'median_income',\n                                                                          'avg_dist_nearest_crimes',\n                                                                          'nearest_school_rating']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['neighname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(n_estimators=10,\n                                                              random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['area', 'bed', 'bath',\n                                                   'median_income',\n                                                   'avg_dist_nearest_crimes',\n                                                   'nearest_school_rating']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['neighname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(n_estimators=10, random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['area', 'bed', 'bath', 'median_income',\n                                  'avg_dist_nearest_crimes',\n                                  'nearest_school_rating']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['neighname'])])num['area', 'bed', 'bath', 'median_income', 'avg_dist_nearest_crimes', 'nearest_school_rating']StandardScalerStandardScaler()cat['neighname']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(n_estimators=10, random_state=42)\nCode\n#! echo: true\n#! code-fold: true\n\ngrid.best_params_\n\n\n{'randomforestregressor__max_depth': 21,\n 'randomforestregressor__n_estimators': 200}\nAfter inputing the best parameters, we improve the model score to around 0.8.\nCode\n#! echo: true\n#! code-fold: true\n\n\n##improved\nimproved_pipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=200, \n                                       random_state=42,\n                                      max_depth=21)\n)\nCode\nimproved_pipe.fit(train_set, y_train);\nCode\nimproved_pipe.score(test_set, y_test)\n\n\n0.8070084375416333"
  },
  {
    "objectID": "exploratory/zillows.html",
    "href": "exploratory/zillows.html",
    "title": "Data Transformation and Analysis",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of 690.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport csv\nfrom selenium.webdriver.chrome.options import Options\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\n## DATA 1 : Zillow Housing Data\n# driver = webdriver.Chrome()\n# base_url = 'https://www.zillow.com/austin-tx/'\n# driver.get(base_url)\n\n#one page\n# address = driver.find_elements(By.XPATH,'//address')\n# price = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\n# area = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\n# bath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\n# bed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\n# address_scraped=[]\n# price_scraped=[]\n# area_scraped = []\n# bath_scraped = []\n# bed_scraped = []\n\n# for result in address:\n#     address_scraped.append(result.text)\n# for result in price:\n#     price_scraped.append(result.text)\n# for result in area:\n#     area_scraped.append(result.text)\n# for result in bath:\n#     bath_scraped.append(result.text)\n# for result in bed:\n#     bed_scraped.append(result.text)\n    \n# with open(\"zillow2.csv\", \"w\") as f:\n#     f.write(\"Address; Price; Area; Bath; Bed\\n\")\n# for i in range(len(address_scraped)):\n#     with open(\"zillow2.csv\", \"a\") as f:\n#         f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n#loop through Zillow Pages\n\n# address_scraped=[]\n# price_scraped=[]\n# area_scraped = []\n# bath_scraped = []\n# bed_scraped = []\n\n# num_pages_to_scrape = 10\n\n# for page_num in range(num_pages_to_scrape):\n#     address = driver.find_elements(By.XPATH,'//address')\n#     price = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\n#     area = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\n#     bath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\n#     bed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\n#     for result in address:\n#         address_scraped.append(result.text)\n#     for result in price:\n#         price_scraped.append(result.text)\n#     for result in area:\n#         area_scraped.append(result.text)\n#     for result in bath:\n#         bath_scraped.append(result.text)\n#     for result in bed:\n#         bed_scraped.append(result.text)\n\n#     next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n#     next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n#     next_button.click()\n    \n#     sleep(10)\n\n# with open(\"zillow.csv\", \"w\") as f:\n#     f.write(\"Address; Price; Area; Bath; Bed\\n\")\n#     for i in range(len(address_scraped)):\n#         f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n# driver.quit()\n\ndef scrape_zillow(start_page, num_pages_to_scrape):\n    options = Options()\n    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n    driver = webdriver.Chrome(options=options)\n\n    base_url = 'https://www.zillow.com/austin-tx/'\n    driver.get(base_url)\n\n    for _ in range(start_page - 1):\n        next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n        next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n        next_button.click()\n        sleep(10)\n\n    scraped_data = []\n\n    for page_num in range(start_page, start_page + num_pages_to_scrape):\n        listings = driver.find_elements(By.XPATH, '//li[contains(@class, \"StyledListCardWrapper-srp__sc-wtsrtn-0\")]')\n        \n        for listing in listings:\n            address = listing.find_element(By.XPATH, './/address').text\n            price = listing.find_element(By.XPATH, './/span[@data-test=\"property-card-price\"]').text\n            area = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b').text\n            bath = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b').text\n            bed = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b').text\n\n            scraped_data.append([address, price, area, bath, bed, page_num])\n\n        next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n        next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n        next_button.click()\n        sleep(10)\n\n    with open(\"zillow2.csv\", \"a\", newline='') as f:\n        writer = csv.writer(f, delimiter=';')\n        if start_page == 1:\n            writer.writerow([\"Address\", \"Price\", \"Area\", \"Bath\", \"Bed\", \"Page\"])\n        writer.writerows(scraped_data)\n\n    driver.quit()\n\nscrape_zillow(start_page=1, num_pages_to_scrape=5)\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport pandas as pd\nimport requests\nhousing_raw = pd.read_csv(\"Zillows.csv\")\nhousing_raw.head(10)\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n\n\n5\nAustin, TX\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n4\n3\n2,709\n\n\n6\nAustin, TX\n5405 Teri Rd, Austin, TX 78744\n$359,000\n4\n2\n1,551\n\n\n7\nAustin, TX\n12400 Cedar St, Austin, TX 78732\n$19,500,000\n6\n13\n15,394\n\n\n8\nAustin, TX\n8114 Meandering Way, Austin, TX 78759\n$850,000\n3\n4\n2,588\n\n\n9\nAustin, TX\n14920 Ben Davis Dr, Austin, TX 78725\n$299,999\n3\n2\n1,464\n\n\n\n\n\n\n\n\n\n\nNext, we perform geocoding using geocoders and Google geocoding API for the scraped addresses to obtain the locations and coordinates of each housing unit. This step is essential for preparing the data for future spatial analysis. After geocoding and tranformations based on the addresses, we acquired a dataset with coordiantes. In the process there were some data loss since the Open Street Mapabd Google map couldn’t find location matches with some addresses.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n## DATA 1 : Geocoding for Zillow Address\nfrom geopy.geocoders import Nominatim\nnom = Nominatim(user_agent=\"housing\")\npd.set_option('display.max_rows', None)\n\n# from geopy.geocoders import Nominatim\n# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n# import pandas as pd\n# import time\n# # housing[\"coordinates\"] = housing[\"Address\"].apply(nom.geocode) \n# def geocode_with_timeout(address):\n#     try:\n#         return nom.geocode(address, timeout=1) \n#     except (GeocoderTimedOut, GeocoderUnavailable):\n#         return None\n\n# housing[\"coordinates\"] = housing[\"Address\"].apply(lambda x: geocode_with_timeout(x))\n# housing[\"longitude\"] = housing[\"coordinates\"].apply(lambda x:x.longitude if x != None else None)\n# housing[\"latitude\"] = housing[\"coordinates\"].apply(lambda x:x.latitude if x != None else None)\n\n#too much data for geopy.geocoders so we shift to Google API\nimport json\n\nAPI_KEY = 'AIzaSyCje1Sey-dEzL7oJt_eHyswe7yyNIZV5nI'\n\ndef get_coordinates(address):\n    GOOGLE_MAPS_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n    params = {\n        'address': address,\n        'key': API_KEY\n    }\n\n    response = requests.get(GOOGLE_MAPS_API_URL, params=params)\n    data = response.json()\n\n    if data['results']:\n        latitude = data['results'][0]['geometry']['location']['lat']\n        longitude = data['results'][0]['geometry']['location']['lng']\n        return latitude, longitude\n    else:\n        return None, None\n\ncoordinates = housing_raw['Address'].apply(get_coordinates)\n\nhousing_raw['Latitude'], housing_raw['Longitude'] = zip(*coordinates)\n\nimport geopandas as gpd\ngeometry = gpd.points_from_xy(housing_raw[\"Longitude\"], housing_raw[\"Latitude\"])\nhousing_raw = gpd.GeoDataFrame(housing_raw, geometry=geometry)\nhousing = housing_raw[housing_raw['Longitude'].notna()].reset_index(drop = True)\nhousing = housing[housing['Bedrooms'].notna()].reset_index(drop = True)\nhousing= housing[housing['Bedrooms'] != \"--\"]\nhousing = housing[housing['Sqft'] != \"--\"]\nhousing = housing[housing['Longitude'] &lt;= 0]\nhousing_geo = housing[housing['Latitude'] &lt;= 32]\nhousing_geo.head()\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nhousing_geo.columns = housing_geo.columns.str.strip()\n\n#to float\nhousing_geo[\"price\"] = housing_geo[\"Price\"].replace('[\\$,+]', \"\", regex=True).astype(float)\nhousing_geo[\"area\"] = housing_geo[\"Sqft\"].replace(\"[\\,]\", \"\", regex=True).astype(float)\nhousing_geo[\"bed\"] = housing_geo[\"Bedrooms\"].astype(float)\nhousing_geo[\"bath\"] = housing_geo[\"Bathrooms\"].astype(float)\n\nhousing_geo.head()\n\n\nD:\\ana\\envs\\environment\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\nprice\narea\nbed\nbath\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n610000.0\n3260.0\n5.0\n4.0\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n350000.0\n1463.0\n3.0\n2.0\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n124900.0\n1568.0\n3.0\n2.0\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n194898.0\n435.0\n1.0\n1.0\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n259900.0\n1084.0\n2.0\n2.0\n\n\n\n\n\n\n\n\n\n\nAfter conducting exploratory analysis on the data pertaining to housing prices and various housing characteristics, it has come to our attention that Austin stands out as one of the cities with relatively higher housing prices in the United States.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n#charts\nimport altair as alt\n\nselected_columns =  ['price', 'area', 'bed', 'Address']\n\nchart = alt.Chart(housing_geo[selected_columns]).mark_circle().encode(\n    x=alt.X('price:Q', axis=alt.Axis(title='Price')),\n    y=alt.Y('area:Q', axis=alt.Axis(title='Size')),\n    color=alt.Color('bed:Q', legend=alt.Legend(title='Bedrooms')),\n    tooltip=['price:Q', 'area:Q', 'bed:Q', 'Address:N']\n).properties(width=600, height=400)\n\nchart.interactive()\n\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\npriceper = pd.DataFrame({'Price per Square Foot': housing_geo['price'] / housing_geo['area']})\n\nchart2 = alt.Chart(priceper).mark_bar(color='#8abddd').encode(\n    alt.X('Price per Square Foot', bin=alt.Bin(maxbins=22), title='Housing Prices per square foot'),\n    y='count()',\n    tooltip=['count()']\n).properties(\n    title='Austin Housing Prices per square foot',\n    height=300,\n    width=600\n)\n\nchart2.interactive()"
  },
  {
    "objectID": "exploratory/zillows.html#zillow-data",
    "href": "exploratory/zillows.html#zillow-data",
    "title": "Data Transformation and Analysis",
    "section": "",
    "text": "To obtain housing prices and other fundamental housing characteristics, we performed web scraping on Zillow’s housing listings near the City of Austin. This involved extracting information such as price, address, size, and amenities for each housing item. The gathered data was then stored in a local CSV file, resulting in a dataset with a size of 690.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport csv\nfrom selenium.webdriver.chrome.options import Options\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\n## DATA 1 : Zillow Housing Data\n# driver = webdriver.Chrome()\n# base_url = 'https://www.zillow.com/austin-tx/'\n# driver.get(base_url)\n\n#one page\n# address = driver.find_elements(By.XPATH,'//address')\n# price = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\n# area = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\n# bath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\n# bed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\n# address_scraped=[]\n# price_scraped=[]\n# area_scraped = []\n# bath_scraped = []\n# bed_scraped = []\n\n# for result in address:\n#     address_scraped.append(result.text)\n# for result in price:\n#     price_scraped.append(result.text)\n# for result in area:\n#     area_scraped.append(result.text)\n# for result in bath:\n#     bath_scraped.append(result.text)\n# for result in bed:\n#     bed_scraped.append(result.text)\n    \n# with open(\"zillow2.csv\", \"w\") as f:\n#     f.write(\"Address; Price; Area; Bath; Bed\\n\")\n# for i in range(len(address_scraped)):\n#     with open(\"zillow2.csv\", \"a\") as f:\n#         f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n#loop through Zillow Pages\n\n# address_scraped=[]\n# price_scraped=[]\n# area_scraped = []\n# bath_scraped = []\n# bed_scraped = []\n\n# num_pages_to_scrape = 10\n\n# for page_num in range(num_pages_to_scrape):\n#     address = driver.find_elements(By.XPATH,'//address')\n#     price = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 fDSTNn\"]//span[@data-test=\"property-card-price\"]')\n#     area = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b')\n#     bath = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b')\n#     bed = driver.find_elements(By.XPATH, '//div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b')\n\n#     for result in address:\n#         address_scraped.append(result.text)\n#     for result in price:\n#         price_scraped.append(result.text)\n#     for result in area:\n#         area_scraped.append(result.text)\n#     for result in bath:\n#         bath_scraped.append(result.text)\n#     for result in bed:\n#         bed_scraped.append(result.text)\n\n#     next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n#     next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n#     next_button.click()\n    \n#     sleep(10)\n\n# with open(\"zillow.csv\", \"w\") as f:\n#     f.write(\"Address; Price; Area; Bath; Bed\\n\")\n#     for i in range(len(address_scraped)):\n#         f.write(str(address_scraped[i])+\"; \"+str(price_scraped[i])+\"; \"+str(area_scraped[i])+\"; \"+str(bath_scraped[i])+\"; \"+str(bed_scraped[i])+\"\\n\")\n\n# driver.quit()\n\ndef scrape_zillow(start_page, num_pages_to_scrape):\n    options = Options()\n    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n    driver = webdriver.Chrome(options=options)\n\n    base_url = 'https://www.zillow.com/austin-tx/'\n    driver.get(base_url)\n\n    for _ in range(start_page - 1):\n        next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n        next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n        next_button.click()\n        sleep(10)\n\n    scraped_data = []\n\n    for page_num in range(start_page, start_page + num_pages_to_scrape):\n        listings = driver.find_elements(By.XPATH, '//li[contains(@class, \"StyledListCardWrapper-srp__sc-wtsrtn-0\")]')\n        \n        for listing in listings:\n            address = listing.find_element(By.XPATH, './/address').text\n            price = listing.find_element(By.XPATH, './/span[@data-test=\"property-card-price\"]').text\n            area = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[3]/b').text\n            bath = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[2]/b').text\n            bed = listing.find_element(By.XPATH, './/div[@class=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\"]/ul/li[1]/b').text\n\n            scraped_data.append([address, price, area, bath, bed, page_num])\n\n        next_button_xpath = '//li[@class=\"PaginationJumpItem-c11n-8-84-3__sc-18wdg2l-0 jRUCrX\"]/a[@title=\"Next page\"]'\n        next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n        next_button.click()\n        sleep(10)\n\n    with open(\"zillow2.csv\", \"a\", newline='') as f:\n        writer = csv.writer(f, delimiter=';')\n        if start_page == 1:\n            writer.writerow([\"Address\", \"Price\", \"Area\", \"Bath\", \"Bed\", \"Page\"])\n        writer.writerows(scraped_data)\n\n    driver.quit()\n\nscrape_zillow(start_page=1, num_pages_to_scrape=5)\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport pandas as pd\nimport requests\nhousing_raw = pd.read_csv(\"Zillows.csv\")\nhousing_raw.head(10)\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n\n\n5\nAustin, TX\n9017 Frostwood Trl, Austin, TX 78729\n$450,000\n4\n3\n2,709\n\n\n6\nAustin, TX\n5405 Teri Rd, Austin, TX 78744\n$359,000\n4\n2\n1,551\n\n\n7\nAustin, TX\n12400 Cedar St, Austin, TX 78732\n$19,500,000\n6\n13\n15,394\n\n\n8\nAustin, TX\n8114 Meandering Way, Austin, TX 78759\n$850,000\n3\n4\n2,588\n\n\n9\nAustin, TX\n14920 Ben Davis Dr, Austin, TX 78725\n$299,999\n3\n2\n1,464\n\n\n\n\n\n\n\n\n\n\nNext, we perform geocoding using geocoders and Google geocoding API for the scraped addresses to obtain the locations and coordinates of each housing unit. This step is essential for preparing the data for future spatial analysis. After geocoding and tranformations based on the addresses, we acquired a dataset with coordiantes. In the process there were some data loss since the Open Street Mapabd Google map couldn’t find location matches with some addresses.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n## DATA 1 : Geocoding for Zillow Address\nfrom geopy.geocoders import Nominatim\nnom = Nominatim(user_agent=\"housing\")\npd.set_option('display.max_rows', None)\n\n# from geopy.geocoders import Nominatim\n# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n# import pandas as pd\n# import time\n# # housing[\"coordinates\"] = housing[\"Address\"].apply(nom.geocode) \n# def geocode_with_timeout(address):\n#     try:\n#         return nom.geocode(address, timeout=1) \n#     except (GeocoderTimedOut, GeocoderUnavailable):\n#         return None\n\n# housing[\"coordinates\"] = housing[\"Address\"].apply(lambda x: geocode_with_timeout(x))\n# housing[\"longitude\"] = housing[\"coordinates\"].apply(lambda x:x.longitude if x != None else None)\n# housing[\"latitude\"] = housing[\"coordinates\"].apply(lambda x:x.latitude if x != None else None)\n\n#too much data for geopy.geocoders so we shift to Google API\nimport json\n\nAPI_KEY = 'AIzaSyCje1Sey-dEzL7oJt_eHyswe7yyNIZV5nI'\n\ndef get_coordinates(address):\n    GOOGLE_MAPS_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n    params = {\n        'address': address,\n        'key': API_KEY\n    }\n\n    response = requests.get(GOOGLE_MAPS_API_URL, params=params)\n    data = response.json()\n\n    if data['results']:\n        latitude = data['results'][0]['geometry']['location']['lat']\n        longitude = data['results'][0]['geometry']['location']['lng']\n        return latitude, longitude\n    else:\n        return None, None\n\ncoordinates = housing_raw['Address'].apply(get_coordinates)\n\nhousing_raw['Latitude'], housing_raw['Longitude'] = zip(*coordinates)\n\nimport geopandas as gpd\ngeometry = gpd.points_from_xy(housing_raw[\"Longitude\"], housing_raw[\"Latitude\"])\nhousing_raw = gpd.GeoDataFrame(housing_raw, geometry=geometry)\nhousing = housing_raw[housing_raw['Longitude'].notna()].reset_index(drop = True)\nhousing = housing[housing['Bedrooms'].notna()].reset_index(drop = True)\nhousing= housing[housing['Bedrooms'] != \"--\"]\nhousing = housing[housing['Sqft'] != \"--\"]\nhousing = housing[housing['Longitude'] &lt;= 0]\nhousing_geo = housing[housing['Latitude'] &lt;= 32]\nhousing_geo.head()\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nhousing_geo.columns = housing_geo.columns.str.strip()\n\n#to float\nhousing_geo[\"price\"] = housing_geo[\"Price\"].replace('[\\$,+]', \"\", regex=True).astype(float)\nhousing_geo[\"area\"] = housing_geo[\"Sqft\"].replace(\"[\\,]\", \"\", regex=True).astype(float)\nhousing_geo[\"bed\"] = housing_geo[\"Bedrooms\"].astype(float)\nhousing_geo[\"bath\"] = housing_geo[\"Bathrooms\"].astype(float)\n\nhousing_geo.head()\n\n\nD:\\ana\\envs\\environment\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\nprice\narea\nbed\nbath\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n610000.0\n3260.0\n5.0\n4.0\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n350000.0\n1463.0\n3.0\n2.0\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n124900.0\n1568.0\n3.0\n2.0\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n194898.0\n435.0\n1.0\n1.0\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n259900.0\n1084.0\n2.0\n2.0\n\n\n\n\n\n\n\n\n\n\nAfter conducting exploratory analysis on the data pertaining to housing prices and various housing characteristics, it has come to our attention that Austin stands out as one of the cities with relatively higher housing prices in the United States.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n#charts\nimport altair as alt\n\nselected_columns =  ['price', 'area', 'bed', 'Address']\n\nchart = alt.Chart(housing_geo[selected_columns]).mark_circle().encode(\n    x=alt.X('price:Q', axis=alt.Axis(title='Price')),\n    y=alt.Y('area:Q', axis=alt.Axis(title='Size')),\n    color=alt.Color('bed:Q', legend=alt.Legend(title='Bedrooms')),\n    tooltip=['price:Q', 'area:Q', 'bed:Q', 'Address:N']\n).properties(width=600, height=400)\n\nchart.interactive()\n\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\npriceper = pd.DataFrame({'Price per Square Foot': housing_geo['price'] / housing_geo['area']})\n\nchart2 = alt.Chart(priceper).mark_bar(color='#8abddd').encode(\n    alt.X('Price per Square Foot', bin=alt.Bin(maxbins=22), title='Housing Prices per square foot'),\n    y='count()',\n    tooltip=['count()']\n).properties(\n    title='Austin Housing Prices per square foot',\n    height=300,\n    width=600\n)\n\nchart2.interactive()"
  },
  {
    "objectID": "exploratory/zillows.html#tract-data",
    "href": "exploratory/zillows.html#tract-data",
    "title": "Data Transformation and Analysis",
    "section": "2. Tract Data",
    "text": "2. Tract Data\nThen we extracted census tract data to provide spatial references in Austin.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n## DATA 2 : Get census tracts \nimport pygris\n\naus_tracts = pygris.tracts(\n    state=\"48\", county=\"453\", year=2021\n)\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\naus_tracts.plot(ax=ax, facecolor='none', edgecolor='black', linestyle='-', lw=0.5)\nplt.title('Austin Census Tract')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exploratory/zillows.html#census-data",
    "href": "exploratory/zillows.html#census-data",
    "title": "Data Transformation and Analysis",
    "section": "3. Census Data",
    "text": "3. Census Data\nTo gain a deeper understanding of how the surrounding neighborhood demographics and characteristics influence housing prices in Austin, we obtained census data from the Census API and spatially joiend with tract data. This data will serve as a valuable resource to explore and analyze the relationships between various demographic factors and housing prices in the area.\n\n3.1 Median Income of Surrounding Tract\nIn our analysis, we prioritized the examination of median income as the initial demographic variable. Median income is a key metric that provides valuable insights into the economic prosperity and financial health of a specific population. By focusing on median income, we aim to understand the distribution of earnings within the community and its potential impact on housing dynamics.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n## DATA 3 : Get demographic data from census api\nimport cenpy\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nincome = acs.varslike(\"MEDIAN HOUSEHOLD INCOME\",\n                      by=\"concept\",).sort_index()\n\nvariables = [\n    \"NAME\",\n    \"B19013A_001E\"]\n\naus_data = acs.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": \"48\", \"county\": \"453\"},\n)\n\nfor variable in variables:\n    if variable != \"NAME\":\n        aus_data[variable] = aus_data[variable].astype(float)\n\nmerged = pandas.merge(aus_data, aus_tracts, left_on=\"tract\", right_on=\"TRACTCE\", how=\"right\")\nmask = merged['B19013A_001E'] &gt;= 0\nmerged = merged[mask]\nmerged = merged.rename(columns={\"B19013A_001E\" : \"median_income\"})\nmerged.head()\n\n\n\n\n\n\n\n\n\nNAME_x\nmedian_income\nstate\ncounty\ntract\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\nNAME_y\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\nCensus Tract 22.11, Travis County, Texas\n83882.0\n48\n453\n002211\n48\n453\n002211\n48453002211\n22.11\nCensus Tract 22.11\nG5020\nS\n6799964\n0\n+30.2742994\n-097.6192562\nPOLYGON ((-97.63835 30.27447, -97.63727 30.276...\n\n\n1\nCensus Tract 25, Travis County, Texas\n85822.0\n48\n453\n002500\n48\n453\n002500\n48453002500\n25\nCensus Tract 25\nG5020\nS\n3419935\n0\n+30.4166759\n-097.7551447\nPOLYGON ((-97.76760 30.41209, -97.76753 30.412...\n\n\n2\nCensus Tract 24.36, Travis County, Texas\n53214.0\n48\n453\n002436\n48\n453\n002436\n48453002436\n24.36\nCensus Tract 24.36\nG5020\nS\n57384988\n13\n+30.1065025\n-097.6591614\nPOLYGON ((-97.69697 30.14765, -97.69691 30.149...\n\n\n3\nCensus Tract 24.32, Travis County, Texas\n110221.0\n48\n453\n002432\n48\n453\n002432\n48453002432\n24.32\nCensus Tract 24.32\nG5020\nS\n44469976\n14350\n+30.1302451\n-097.7195595\nPOLYGON ((-97.75577 30.10118, -97.75562 30.101...\n\n\n4\nCensus Tract 21.12, Travis County, Texas\n80553.0\n48\n453\n002112\n48\n453\n002112\n48453002112\n21.12\nCensus Tract 21.12\nG5020\nS\n1703455\n0\n+30.3139236\n-097.6825978\nPOLYGON ((-97.69126 30.31622, -97.69081 30.316...\n\n\n\n\n\n\n\nNext we joined the housing locations to the demographic dataset, with the two inforamtion layered, we can observe certain patterns.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\n## Join housisng data to the tract it belongs to\nmerged = gpd.GeoDataFrame(merged, geometry = 'geometry')\n\nhousing_geo.crs = \"EPSG:4326\"\nmerged = merged.to_crs(housing_geo.crs)\nhousing_dem_geo = gpd.sjoin(housing_geo, merged, how=\"left\", op=\"within\")\n\nbase = alt.Chart(merged).mark_geoshape(\n    fill='none',\n    stroke='black',\n    strokeWidth=0.5\n).project('mercator').properties(\n    width=600,\n    height=400\n)\n\npoints = alt.Chart(housing_dem_geo).mark_circle(\n    size=30,\n    opacity=0.8,\n    stroke='black',\n    strokeWidth=0.3\n).encode(\n    longitude='Longitude:Q',\n    latitude='Latitude:Q',\n    color=alt.Color('price:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['price:Q']\n).interactive()\n\nfinal_chart = base + points\n\nfinal_chart\n\n\nD:\\ana\\envs\\environment\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nmerged.plot(ax=ax, \n            column='median_income', \n            cmap='Blues', \n            legend=True, \n            edgecolor='black', \n            linestyle='-', \n            lw=0.5, \n            legend_kwds={'label': \"Median Income\"})\n\nhousing_dem_geo.plot(ax=ax, \n                     column='price', \n                     cmap='RdPu', \n                     legend=True, \n                     edgecolor='black', \n                     linestyle='-', \n                     lw=0.2, \n                     markersize=6,\n                     vmax=1400000, \n                     legend_kwds={'label': \"Price\"})\n\nplt.title('Housing Price and Neighborhood Median Income')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exploratory/zillows.html#crime-data-average-distance-to-violent-crime-incidents",
    "href": "exploratory/zillows.html#crime-data-average-distance-to-violent-crime-incidents",
    "title": "Data Transformation and Analysis",
    "section": "4. Crime Data : Average Distance to Violent Crime Incidents",
    "text": "4. Crime Data : Average Distance to Violent Crime Incidents\nIn our analysis of housing prices, we find it also essential to take into account crime reports, recognizing the potential influence of crime incidents in the surrounding neighborhood on property values. Based on the 2022 crime report data, we calculated the average distance from violent crime incidents for each of the housing item.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport geopandas as gpd\nimport pandas as pd\ncrime_df = pd.read_csv(\"Crime_Reports_2022.csv\", low_memory=False)\n\nfrom shapely.geometry import Point\ngeometry = [Point(xy) for xy in zip(crime_df['Longitude'], crime_df['Latitude'])]\ncrime_gdf = gpd.GeoDataFrame(crime_df, geometry=geometry)\n\nfrom shapely.ops import nearest_points\n\ndef average_distance_nearest_crimes(house, crime_gdf, num_neighbors=5):\n    distances = crime_gdf.geometry.distance(house.geometry)\n\n    nearest_distances = distances.nsmallest(num_neighbors)\n\n    return nearest_distances.mean()\n\nhousing_dem_geo['avg_dist_nearest_crimes'] = housing_dem_geo.apply(\n    lambda x: average_distance_nearest_crimes(x, crime_gdf),\n    axis=1\n)\n\nhousing_dem_geo.head()\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\nprice\n...\nGEOID\nNAME_y\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\navg_dist_nearest_crimes\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n610000.0\n...\n48453002438\n24.38\nCensus Tract 24.38\nG5020\nS\n5440575.0\n0.0\n+30.1620667\n-097.8126155\n0.005493\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n350000.0\n...\n48453002448\n24.48\nCensus Tract 24.48\nG5020\nS\n16064158.0\n0.0\n+30.1919584\n-097.7152738\n0.007604\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n124900.0\n...\n48453002430\n24.30\nCensus Tract 24.30\nG5020\nS\n2233372.0\n0.0\n+30.1717783\n-097.7547761\n0.005222\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n194898.0\n...\n48453001312\n13.12\nCensus Tract 13.12\nG5020\nS\n1968315.0\n0.0\n+30.2484230\n-097.7579482\n0.004489\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n259900.0\n...\n48453002440\n24.40\nCensus Tract 24.40\nG5020\nS\n10890257.0\n2757.0\n+30.1448973\n-097.7480990\n0.010090\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "exploratory/zillows.html#school-data-location-and-rating",
    "href": "exploratory/zillows.html#school-data-location-and-rating",
    "title": "Data Transformation and Analysis",
    "section": "5. School Data : Location and Rating",
    "text": "5. School Data : Location and Rating\nThe location and quality of schools are crucial factors for homebuyers to consider, as they may significantly impact housing prices. We recognize the importance of this aspect and incorporate it into our analysis, understanding that the reputation and performance of schools in an area can have a substantial influence on the overall desirability and, consequently, the market value of residential properties.\nWe used Google API to query the school data and conduct analysis.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nimport requests\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef get_schools_data(api_key):\n    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n    query = \"high schools in Austin, Texas\"\n    params = {\n        'query': query,\n        'key': api_key\n    }\n\n    response = requests.get(url, params=params)\n    results = response.json().get('results', [])\n\n    schools_data = []\n    for place in results:\n        name = place.get('name')\n        rating = place.get('rating', 'No rating')\n        lat = place['geometry']['location']['lat']\n        lng = place['geometry']['location']['lng']\n        schools_data.append({\"name\": name, \"rating\": rating, \"geometry\": Point(lng, lat)})\n\n    return schools_data\n\napi_key = 'AIzaSyA9OoLSvcATft9gBE3aYK3JhIIlUdYtJJQ'\nschools_data = get_schools_data(api_key)\n\ngdf = gpd.GeoDataFrame(schools_data, crs=\"EPSG:4326\")\noutput_filename = 'schools_in_austin.geojson'\ngdf.to_file(output_filename, driver='GeoJSON')\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nschools = gpd.read_file(\"schools_in_austin.geojson\")\n\nfiltered_schools = schools[schools['rating'] &gt; 0]\nfiltered_schools\n\n\n\n\n\n\n\n\n\nname\nrating\ngeometry\n\n\n\n\n0\nAnderson High School\n3.5\nPOINT (-97.75336 30.37614)\n\n\n1\nAustin High School\n4.5\nPOINT (-97.76673 30.27356)\n\n\n2\nHyde Park High School\n4.3\nPOINT (-97.73003 30.40817)\n\n\n3\nMcCallum High School\n4.3\nPOINT (-97.73037 30.32555)\n\n\n4\nNavarro Early College High School\n4.5\nPOINT (-97.70773 30.36091)\n\n\n5\nJames Bowie High School\n2.6\nPOINT (-97.85850 30.18765)\n\n\n6\nCrockett High School\n4.3\nPOINT (-97.79682 30.21340)\n\n\n7\nPremier High School - Austin South\n3.7\nPOINT (-97.78344 30.22657)\n\n\n8\nEastside Early College High School\n3.9\nPOINT (-97.70936 30.26887)\n\n\n9\nLBJ High School\n4.0\nPOINT (-97.65667 30.31349)\n\n\n11\nWestlake High School\n3.9\nPOINT (-97.81253 30.27562)\n\n\n12\nWestwood High School\n3.7\nPOINT (-97.79776 30.45615)\n\n\n13\nNortheast Early College High School\n3.9\nPOINT (-97.68958 30.32250)\n\n\n14\nCedars International Next Generation High School\n4.7\nPOINT (-97.70911 30.32777)\n\n\n15\nMcNeil High School\n3.3\nPOINT (-97.73325 30.44927)\n\n\n16\nPremier High School - Austin Wells Branch\n3.4\nPOINT (-97.69689 30.43794)\n\n\n17\nWilliam B. Travis High School\n3.6\nPOINT (-97.74419 30.23297)\n\n\n18\nKIPP Austin Brave High School\n2.6\nPOINT (-97.75712 30.20246)\n\n\n19\nSan Juan Diego Catholic High School\n1.0\nPOINT (-97.76166 30.24069)\n\n\n\n\n\n\n\nThe following map shows the crime incidents and school locations layered with housing locations.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\ncrime_gdf.crs = \"EPSG:4326\"\n\nfig, ax = plt.subplots(figsize=(8,8))\n\naus_tracts.plot(ax=ax, color='none', edgecolor='black', lw=0.2,label='Census Tracts')\n\ncrime_gdf.plot(ax=ax, color='#df6f85', markersize=1, label='Crime Data')\nhousing_dem_geo.plot(ax=ax, color='#8abddd', edgecolor='#8abddd', markersize=5, label='Housing')\nfiltered_schools.plot(ax=ax, color='#f5ae52', edgecolor='#f5ae52', markersize=16, label='Filtered Schools')\n\nax.set_title('Schools, Crime Data on Census Tract Map')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\nax.legend()\n\nplt.show()\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2452\\292801629.py:18: UserWarning: Legend does not support handles for PatchCollection instances.\nSee: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#implementing-a-custom-legend-handler\n  ax.legend()\n\n\n\n\n\nNext we calculated the ratings of the nearest school of each housing item.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\ndef nearest_school_rating(house, schools_gdf):\n    nearest_school = schools_gdf.geometry == nearest_points(house.geometry, schools_gdf.unary_union)[1]\n    school_rating = schools_gdf[nearest_school]['rating'].iloc[0]\n    return school_rating\n\nhousing_dem_geo['nearest_school_rating'] = housing_dem_geo.apply(nearest_school_rating, schools_gdf=filtered_schools, axis=1)\nhousing_dem_geo.head()\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\nprice\n...\nNAME_y\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\navg_dist_nearest_crimes\nnearest_school_rating\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n610000.0\n...\n24.38\nCensus Tract 24.38\nG5020\nS\n5440575.0\n0.0\n+30.1620667\n-097.8126155\n0.005493\n2.6\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n350000.0\n...\n24.48\nCensus Tract 24.48\nG5020\nS\n16064158.0\n0.0\n+30.1919584\n-097.7152738\n0.007604\n3.6\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n124900.0\n...\n24.30\nCensus Tract 24.30\nG5020\nS\n2233372.0\n0.0\n+30.1717783\n-097.7547761\n0.005222\n2.6\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n194898.0\n...\n13.12\nCensus Tract 13.12\nG5020\nS\n1968315.0\n0.0\n+30.2484230\n-097.7579482\n0.004489\n1.0\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n259900.0\n...\n24.40\nCensus Tract 24.40\nG5020\nS\n10890257.0\n2757.0\n+30.1448973\n-097.7480990\n0.010090\n2.6\n\n\n\n\n5 rows × 33 columns"
  },
  {
    "objectID": "exploratory/zillows.html#neighborhood-data",
    "href": "exploratory/zillows.html#neighborhood-data",
    "title": "Data Transformation and Analysis",
    "section": "5. Neighborhood Data",
    "text": "5. Neighborhood Data\nThe final variable is the categorical variable of the specific neighborhood to which each housing item is assigned. This choice is driven by our hypothesis that the neighborhood itself may wield a discernible impact on the pricing dynamics of housing units. Recognizing the diverse characteristics, amenities, and local influences associated with different neighborhoods, we aim to investigate how this categorical factor contributes to the variability in housing prices.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nneighborhood = gpd.read_file(\"Neighborhoods.geojson\")\nneighborhood.crs = \"EPSG:4326\"\nneighborhood = neighborhood.drop(columns=[\"fid\",\"shape_area\",\"sqmiles\",\"shape_leng\",\"shape_length\",\"target_fid\"])\n\nfig, ax = plt.subplots(figsize=(7, 7))  # You can adjust the figure size as needed\nneighborhood.plot(ax=ax,color='#8abddd')\n\n# Adding titles and labels (optional)\nax.set_title('Neighborhoods Map')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nhousing_dem_geo = housing_dem_geo.drop(columns=['index_right'])\nhousing_dem_geo = gpd.sjoin(housing_dem_geo, neighborhood, how=\"left\", op=\"within\")\n\nhousing_dem_geo.head()\n\n\nD:\\ana\\envs\\environment\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\n\n\n\n\n\nArea\nAddress\nPrice\nBedrooms\nBathrooms\nSqft\nLatitude\nLongitude\ngeometry\nprice\n...\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\navg_dist_nearest_crimes\nnearest_school_rating\nindex_right\nneighname\n\n\n\n\n0\nAustin, TX\n602 Kingfisher Creek Dr, Austin, TX 78748\n$610,000\n5\n4\n3,260\n30.149983\n-97.814085\nPOINT (-97.81409 30.14998)\n610000.0\n...\nG5020\nS\n5440575.0\n0.0\n+30.1620667\n-097.8126155\n0.005493\n2.6\n92.0\nSLAUGHTER CREEK\n\n\n1\nAustin, TX\n3504 Black Granite Dr, Austin, TX 78744\n$350,000\n3\n2\n1,463\n30.202842\n-97.697146\nPOINT (-97.69715 30.20284)\n350000.0\n...\nG5020\nS\n16064158.0\n0.0\n+30.1919584\n-097.7152738\n0.007604\n3.6\n10.0\nSOUTHEAST\n\n\n2\nAustin, TX\n2512 Sunny Hills Dr #27, Austin, TX 78744\n$124,900\n3\n2\n1,568\n30.173736\n-97.763961\nPOINT (-97.76396 30.17374)\n124900.0\n...\nG5020\nS\n2233372.0\n0.0\n+30.1717783\n-097.7547761\n0.005222\n2.6\n35.0\nBLUFF SPRINGS\n\n\n3\nAustin, TX\n1600 S 1st St #315, Austin, TX 78704\n$194,898\n1\n1\n435\n30.249127\n-97.755731\nPOINT (-97.75573 30.24913)\n194898.0\n...\nG5020\nS\n1968315.0\n0.0\n+30.2484230\n-097.7579482\n0.004489\n1.0\n8.0\nBOULDIN CREEK\n\n\n4\nAustin, TX\n8517 Deja Ave, Austin, TX 78747\n$259,900\n2\n2\n1,084\n30.151030\n-97.737490\nPOINT (-97.73749 30.15103)\n259900.0\n...\nG5020\nS\n10890257.0\n2757.0\n+30.1448973\n-097.7480990\n0.010090\n2.6\n35.0\nBLUFF SPRINGS\n\n\n\n\n5 rows × 34 columns\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nhousing_dem_geo.to_file(\"final_dataset.geojson\", driver=\"GeoJSON\")"
  },
  {
    "objectID": "Modeling/1-modelbuild.html#calculate-the-percent-error",
    "href": "Modeling/1-modelbuild.html#calculate-the-percent-error",
    "title": "Modeling",
    "section": "Calculate the Percent Error",
    "text": "Calculate the Percent Error\nNext we calculate the percent error for each of the housing item to gain a more direct understanding of the model performance.\n\n\nCode\n#! echo: true\n#! code-fold: true\nimport pandas as pd\n\nX_test_df = pd.DataFrame(X_test,columns = feature_cols)\nX_test_df.head()\n\n\n\n\n\n\n\n\n\narea\nbed\nbath\nmedian_income\navg_dist_nearest_crimes\nnearest_school_rating\nneighname\n\n\n\n\n158\n1239.0\n3.0\n3.0\n80553.0\n0.002930\n4.0\nWINDSOR PARK\n\n\n180\n1001.0\n2.0\n2.0\n63464.0\n0.002402\n4.5\nWOOTEN\n\n\n205\n2039.0\n3.0\n3.0\n108529.0\n0.004444\n4.5\nCRESTVIEW\n\n\n320\n1469.0\n3.0\n2.0\n96071.0\n0.007518\n2.6\nBRODIE LANE\n\n\n117\n1487.0\n3.0\n2.0\n73676.0\n0.004060\n4.3\nDITTMAR--SLAUGHTER\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\nlog_predictions = pipe.predict(X_test)\npredictions = np.exp(log_predictions)\ny_test_exp = np.exp(y_test)\n\npercent_error = ((predictions - y_test_exp) / y_test_exp) * 100\n\nerror_per = pd.DataFrame({'Actual_Sale_Price': y_test_exp, 'Predicted_Sale_Price': predictions, 'Percent_Error': percent_error})\nerror_per.head()\n\n\n\n\n\n\n\n\n\nActual_Sale_Price\nPredicted_Sale_Price\nPercent_Error\n\n\n\n\n158\n304900.0\n4.602214e+05\n50.941748\n\n\n180\n255000.0\n2.991891e+05\n17.329058\n\n\n205\n815000.0\n1.315205e+06\n61.374811\n\n\n320\n585000.0\n4.091233e+05\n-30.064400\n\n\n117\n435000.0\n5.058989e+05\n16.298597\n\n\n\n\n\n\n\nNext we group the housing by neighborhood to explore and better visualize how our model are performing spatially.\n\n\nCode\n#! echo: true\n#! code-fold: true\nextract = rf_housing.loc[error_per.index, ['geometry', 'neighname', 'price']]\nextract['percent_error'] = percent_error \n\nmedian_percent_error = extract.groupby('neighname')['percent_error'].median().reset_index()\nmedian_percent_error.head(10)\n\n\n\n\n\n\n\n\n\nneighname\npercent_error\n\n\n\n\n0\nALLANDALE\n-13.649738\n\n\n1\nANDERSON MILL\n24.304395\n\n\n2\nBARTON CREEK MALL\n-37.972775\n\n\n3\nBARTON HILLS\n3.018983\n\n\n4\nBLUFF SPRINGS\n34.187118\n\n\n5\nBOULDIN CREEK\n-15.291938\n\n\n6\nBRENTWOOD\n2.552893\n\n\n7\nBRODIE LANE\n-23.525161\n\n\n8\nBULL CREEK\n-0.738321\n\n\n9\nCENTRAL EAST AUSTIN\n-30.211868\n\n\n\n\n\n\n\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nneighborhood = gpd.read_file(\"../exploratory/Neighborhoods.geojson\")\nneighborhood.crs = \"EPSG:4326\"\nneighborhood = neighborhood.drop(columns=[\"fid\",\"shape_area\",\"sqmiles\",\"shape_leng\",\"shape_length\",\"target_fid\"])\n\npercentbynei = neighborhood.merge(median_percent_error, left_on='neighname', right_on='neighname')\n\nimport altair as alt\nimport geopandas as gpd\n\nchart = alt.Chart(percentbynei).mark_geoshape(\n    stroke='black',\n    strokeWidth=0.4\n).encode(\n    color=alt.Color('percent_error:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['percent_error:Q']\n).project('mercator').properties(\n    width=700,\n    height=500,\n    title='Median Percent Error by Neighborhood'\n)\n\nchart.interactive()\n\n\n\n\n\n\n\n\nAs indicated by the map, our model performs better in the central parts of Austin than in the peripheral neighborhoods, with an median percent error of 5.56 for each housing item.\n\n\nCode\n#! echo: true\n#! code-fold: true\n\nmedian_error = error_per['Percent_Error'].median()\nmedian_error\n\n\n5.569748035342189"
  }
]